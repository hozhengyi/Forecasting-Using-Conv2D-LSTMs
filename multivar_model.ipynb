import tensorflow_addons as tfa
import numpy as np
import tensorflow as tf
import os


window_size = 168
batch_size = 128
shuffle_buffer = 180000 #greater than or equal to dataset
train = 173000  
prediction_steps = 24

def windowed_dataset(data, window_size, batch_size, shuffle_buffer,max_rhum,min_rhum):
    dataset = tf.data.Dataset.from_tensor_slices(data) #take each row and slice
    dataset = dataset.window(window_size + 24, shift=1, drop_remainder=True) # +1 is the prediction
    dataset = dataset.flat_map(lambda window: window.batch(window_size + 24))
    dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-24,:], (window[-24:,0:1]*(max_rhum-min_rhum))+ min_rhum))
    dataset = dataset.batch(batch_size).prefetch(20)
    return dataset

tensor5d2f = np.load('5DTensorALLVARS.npy')[:,:,4,4]
tensor5d2f = tensor5d2f.reshape((192715,4))
#normalization
maxrhum= 100
minrhum = 0

tensor5d2f[:,0] = (tensor5d2f[:,0]- minrhum ) / ( maxrhum-minrhum)
tensor5d2f[:,1] = (tensor5d2f[:,1]- tensor5d2f[:,1].min() ) / ( tensor5d2f[:,1].max()-tensor5d2f[:,1].min())
tensor5d2f[:,2] = (tensor5d2f[:,2]- tensor5d2f[:,2].min() ) / ( tensor5d2f[:,2].max()-tensor5d2f[:,2].min())
tensor5d2f[:,3] = (tensor5d2f[:,3]- tensor5d2f[:,3].min() ) / ( tensor5d2f[:,3].max()-tensor5d2f[:,3].min())

rhum_train = tensor5d2f[:train,:]
rhum_train = rhum_train.reshape((173000,4))

rhum_valid = tensor5d2f[train:,:]
rhum_valid = rhum_valid.reshape((19715,4))

assert len(rhum_train) + len(rhum_valid) ==  len(tensor5d2f)
assert (tensor5d2f>=0).all() and (tensor5d2f<=1).all()

rhum_train_window = windowed_dataset(rhum_train, window_size,batch_size,shuffle_buffer,maxrhum,minrhum)
rhum_valid_window = windowed_dataset(rhum_valid, window_size,batch_size,shuffle_buffer,maxrhum,minrhum) 

tf.keras.backend.clear_session()
csv_save_to = 'multivar_v3.csv'

csvLogger = tf.keras.callbacks.CSVLogger(csv_save_to, separator=",", append=True)
tblogs_cb = tf.keras.callbacks.TensorBoard(log_dir='multivar_log',write_graph=False, profile_batch=0)

compileConfig = {'optimizer':tf.keras.optimizers.Nadam(), 'loss':tf.keras.losses.Huber(),'metrics':['mse','mae','mape']}

fitConfig = {'x':rhum_train_window,'epochs':200,'validation_data':rhum_valid_window,'callbacks':[csvLogger,tblogs_cb]}


model_1 = tf.keras.models.Sequential([
    tf.keras.layers.InputLayer(input_shape=(window_size,4)),
    tf.keras.layers.LSTM(30,return_sequences=True),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.1,noise_shape = (1,30)),
    tf.keras.layers.LSTM(20,return_sequences=False),
    tf.keras.layers.RepeatVector(prediction_steps),
    tf.keras.layers.LSTM(20,return_sequences=True),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.1,noise_shape = (1,20)),
    tf.keras.layers.LSTM(30,return_sequences=True),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.1,noise_shape = (1,30)),
    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(40,activation='tanh')),
    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(30,activation='tanh')),
    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1)),
    tf.keras.layers.Lambda(lambda x: (x*100))
])

model_1.compile(**compileConfig)
model_1.summary()

tf.keras.backend.clear_session()
rhum_history_1 = model_1.fit(**fitConfig)
model_1.save(model_save_to) 

